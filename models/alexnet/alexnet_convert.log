WARNING: Logging before InitGoogleLogging() is written to STDERR
W1026 16:03:09.965867  3288 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W1026 16:03:09.965906  3288 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W1026 16:03:09.965914  3288 _caffe.cpp:142] Net('alexnet.prototxt', 1, weights='alexnet.caffemodel')
I1026 16:03:09.969400  3288 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 10
      dim: 3
      dim: 227
      dim: 227
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
  }
}
layer {
  name: "prob"
  type: "Softmax"
  bottom: "fc8"
  top: "prob"
}
I1026 16:03:09.969491  3288 layer_factory.hpp:77] Creating layer data
I1026 16:03:09.969503  3288 net.cpp:84] Creating Layer data
I1026 16:03:09.969508  3288 net.cpp:380] data -> data
I1026 16:03:09.969527  3288 net.cpp:122] Setting up data
I1026 16:03:09.969537  3288 net.cpp:129] Top shape: 10 3 227 227 (1545870)
I1026 16:03:09.969540  3288 net.cpp:137] Memory required for data: 6183480
I1026 16:03:09.969543  3288 layer_factory.hpp:77] Creating layer conv1
I1026 16:03:09.969552  3288 net.cpp:84] Creating Layer conv1
I1026 16:03:09.969554  3288 net.cpp:406] conv1 <- data
I1026 16:03:09.969560  3288 net.cpp:380] conv1 -> conv1
I1026 16:03:09.971339  3288 net.cpp:122] Setting up conv1
I1026 16:03:09.971365  3288 net.cpp:129] Top shape: 10 96 55 55 (2904000)
I1026 16:03:09.971372  3288 net.cpp:137] Memory required for data: 17799480
I1026 16:03:09.971391  3288 layer_factory.hpp:77] Creating layer relu1
I1026 16:03:09.971410  3288 net.cpp:84] Creating Layer relu1
I1026 16:03:09.971417  3288 net.cpp:406] relu1 <- conv1
I1026 16:03:09.971427  3288 net.cpp:367] relu1 -> conv1 (in-place)
I1026 16:03:09.971439  3288 net.cpp:122] Setting up relu1
I1026 16:03:09.971447  3288 net.cpp:129] Top shape: 10 96 55 55 (2904000)
I1026 16:03:09.971453  3288 net.cpp:137] Memory required for data: 29415480
I1026 16:03:09.971458  3288 layer_factory.hpp:77] Creating layer norm1
I1026 16:03:09.971468  3288 net.cpp:84] Creating Layer norm1
I1026 16:03:09.971473  3288 net.cpp:406] norm1 <- conv1
I1026 16:03:09.971484  3288 net.cpp:380] norm1 -> norm1
I1026 16:03:09.971499  3288 net.cpp:122] Setting up norm1
I1026 16:03:09.971511  3288 net.cpp:129] Top shape: 10 96 55 55 (2904000)
I1026 16:03:09.971516  3288 net.cpp:137] Memory required for data: 41031480
I1026 16:03:09.971523  3288 layer_factory.hpp:77] Creating layer pool1
I1026 16:03:09.971532  3288 net.cpp:84] Creating Layer pool1
I1026 16:03:09.971539  3288 net.cpp:406] pool1 <- norm1
I1026 16:03:09.971546  3288 net.cpp:380] pool1 -> pool1
I1026 16:03:09.971561  3288 net.cpp:122] Setting up pool1
I1026 16:03:09.971570  3288 net.cpp:129] Top shape: 10 96 27 27 (699840)
I1026 16:03:09.971575  3288 net.cpp:137] Memory required for data: 43830840
I1026 16:03:09.971580  3288 layer_factory.hpp:77] Creating layer conv2
I1026 16:03:09.971596  3288 net.cpp:84] Creating Layer conv2
I1026 16:03:09.971604  3288 net.cpp:406] conv2 <- pool1
I1026 16:03:09.971613  3288 net.cpp:380] conv2 -> conv2
I1026 16:03:09.972612  3288 net.cpp:122] Setting up conv2
I1026 16:03:09.972627  3288 net.cpp:129] Top shape: 10 256 27 27 (1866240)
I1026 16:03:09.972633  3288 net.cpp:137] Memory required for data: 51295800
I1026 16:03:09.972645  3288 layer_factory.hpp:77] Creating layer relu2
I1026 16:03:09.972654  3288 net.cpp:84] Creating Layer relu2
I1026 16:03:09.972661  3288 net.cpp:406] relu2 <- conv2
I1026 16:03:09.972668  3288 net.cpp:367] relu2 -> conv2 (in-place)
I1026 16:03:09.972677  3288 net.cpp:122] Setting up relu2
I1026 16:03:09.972685  3288 net.cpp:129] Top shape: 10 256 27 27 (1866240)
I1026 16:03:09.972692  3288 net.cpp:137] Memory required for data: 58760760
I1026 16:03:09.972697  3288 layer_factory.hpp:77] Creating layer norm2
I1026 16:03:09.972704  3288 net.cpp:84] Creating Layer norm2
I1026 16:03:09.972710  3288 net.cpp:406] norm2 <- conv2
I1026 16:03:09.972721  3288 net.cpp:380] norm2 -> norm2
I1026 16:03:09.972734  3288 net.cpp:122] Setting up norm2
I1026 16:03:09.972743  3288 net.cpp:129] Top shape: 10 256 27 27 (1866240)
I1026 16:03:09.972748  3288 net.cpp:137] Memory required for data: 66225720
I1026 16:03:09.972754  3288 layer_factory.hpp:77] Creating layer pool2
I1026 16:03:09.972764  3288 net.cpp:84] Creating Layer pool2
I1026 16:03:09.972772  3288 net.cpp:406] pool2 <- norm2
I1026 16:03:09.972780  3288 net.cpp:380] pool2 -> pool2
I1026 16:03:09.972793  3288 net.cpp:122] Setting up pool2
I1026 16:03:09.972805  3288 net.cpp:129] Top shape: 10 256 13 13 (432640)
I1026 16:03:09.972810  3288 net.cpp:137] Memory required for data: 67956280
I1026 16:03:09.972815  3288 layer_factory.hpp:77] Creating layer conv3
I1026 16:03:09.972828  3288 net.cpp:84] Creating Layer conv3
I1026 16:03:09.972836  3288 net.cpp:406] conv3 <- pool2
I1026 16:03:09.972844  3288 net.cpp:380] conv3 -> conv3
I1026 16:03:09.975590  3288 net.cpp:122] Setting up conv3
I1026 16:03:09.975607  3288 net.cpp:129] Top shape: 10 384 13 13 (648960)
I1026 16:03:09.975613  3288 net.cpp:137] Memory required for data: 70552120
I1026 16:03:09.975626  3288 layer_factory.hpp:77] Creating layer relu3
I1026 16:03:09.975636  3288 net.cpp:84] Creating Layer relu3
I1026 16:03:09.975641  3288 net.cpp:406] relu3 <- conv3
I1026 16:03:09.975649  3288 net.cpp:367] relu3 -> conv3 (in-place)
I1026 16:03:09.975662  3288 net.cpp:122] Setting up relu3
I1026 16:03:09.975670  3288 net.cpp:129] Top shape: 10 384 13 13 (648960)
I1026 16:03:09.975677  3288 net.cpp:137] Memory required for data: 73147960
I1026 16:03:09.975682  3288 layer_factory.hpp:77] Creating layer conv4
I1026 16:03:09.975692  3288 net.cpp:84] Creating Layer conv4
I1026 16:03:09.975697  3288 net.cpp:406] conv4 <- conv3
I1026 16:03:09.975706  3288 net.cpp:380] conv4 -> conv4
I1026 16:03:09.977783  3288 net.cpp:122] Setting up conv4
I1026 16:03:09.977798  3288 net.cpp:129] Top shape: 10 384 13 13 (648960)
I1026 16:03:09.977804  3288 net.cpp:137] Memory required for data: 75743800
I1026 16:03:09.977814  3288 layer_factory.hpp:77] Creating layer relu4
I1026 16:03:09.977823  3288 net.cpp:84] Creating Layer relu4
I1026 16:03:09.977828  3288 net.cpp:406] relu4 <- conv4
I1026 16:03:09.977839  3288 net.cpp:367] relu4 -> conv4 (in-place)
I1026 16:03:09.977849  3288 net.cpp:122] Setting up relu4
I1026 16:03:09.977855  3288 net.cpp:129] Top shape: 10 384 13 13 (648960)
I1026 16:03:09.977861  3288 net.cpp:137] Memory required for data: 78339640
I1026 16:03:09.977866  3288 layer_factory.hpp:77] Creating layer conv5
I1026 16:03:09.977876  3288 net.cpp:84] Creating Layer conv5
I1026 16:03:09.977882  3288 net.cpp:406] conv5 <- conv4
I1026 16:03:09.977891  3288 net.cpp:380] conv5 -> conv5
I1026 16:03:09.979279  3288 net.cpp:122] Setting up conv5
I1026 16:03:09.979291  3288 net.cpp:129] Top shape: 10 256 13 13 (432640)
I1026 16:03:09.979297  3288 net.cpp:137] Memory required for data: 80070200
I1026 16:03:09.979310  3288 layer_factory.hpp:77] Creating layer relu5
I1026 16:03:09.979318  3288 net.cpp:84] Creating Layer relu5
I1026 16:03:09.979324  3288 net.cpp:406] relu5 <- conv5
I1026 16:03:09.979336  3288 net.cpp:367] relu5 -> conv5 (in-place)
I1026 16:03:09.979343  3288 net.cpp:122] Setting up relu5
I1026 16:03:09.979351  3288 net.cpp:129] Top shape: 10 256 13 13 (432640)
I1026 16:03:09.979357  3288 net.cpp:137] Memory required for data: 81800760
I1026 16:03:09.979362  3288 layer_factory.hpp:77] Creating layer pool5
I1026 16:03:09.979372  3288 net.cpp:84] Creating Layer pool5
I1026 16:03:09.979377  3288 net.cpp:406] pool5 <- conv5
I1026 16:03:09.979385  3288 net.cpp:380] pool5 -> pool5
I1026 16:03:09.979398  3288 net.cpp:122] Setting up pool5
I1026 16:03:09.979408  3288 net.cpp:129] Top shape: 10 256 6 6 (92160)
I1026 16:03:09.979413  3288 net.cpp:137] Memory required for data: 82169400
I1026 16:03:09.979418  3288 layer_factory.hpp:77] Creating layer fc6
I1026 16:03:09.979430  3288 net.cpp:84] Creating Layer fc6
I1026 16:03:09.979436  3288 net.cpp:406] fc6 <- pool5
I1026 16:03:09.979449  3288 net.cpp:380] fc6 -> fc6
I1026 16:03:10.078315  3288 net.cpp:122] Setting up fc6
I1026 16:03:10.078346  3288 net.cpp:129] Top shape: 10 4096 (40960)
I1026 16:03:10.078351  3288 net.cpp:137] Memory required for data: 82333240
I1026 16:03:10.078364  3288 layer_factory.hpp:77] Creating layer relu6
I1026 16:03:10.078377  3288 net.cpp:84] Creating Layer relu6
I1026 16:03:10.078383  3288 net.cpp:406] relu6 <- fc6
I1026 16:03:10.078392  3288 net.cpp:367] relu6 -> fc6 (in-place)
I1026 16:03:10.078402  3288 net.cpp:122] Setting up relu6
I1026 16:03:10.078408  3288 net.cpp:129] Top shape: 10 4096 (40960)
I1026 16:03:10.078411  3288 net.cpp:137] Memory required for data: 82497080
I1026 16:03:10.078414  3288 layer_factory.hpp:77] Creating layer drop6
I1026 16:03:10.078423  3288 net.cpp:84] Creating Layer drop6
I1026 16:03:10.078426  3288 net.cpp:406] drop6 <- fc6
I1026 16:03:10.078433  3288 net.cpp:367] drop6 -> fc6 (in-place)
I1026 16:03:10.078441  3288 net.cpp:122] Setting up drop6
I1026 16:03:10.078446  3288 net.cpp:129] Top shape: 10 4096 (40960)
I1026 16:03:10.078450  3288 net.cpp:137] Memory required for data: 82660920
I1026 16:03:10.078454  3288 layer_factory.hpp:77] Creating layer fc7
I1026 16:03:10.078462  3288 net.cpp:84] Creating Layer fc7
I1026 16:03:10.078466  3288 net.cpp:406] fc7 <- fc6
I1026 16:03:10.078474  3288 net.cpp:380] fc7 -> fc7
I1026 16:03:10.113302  3288 net.cpp:122] Setting up fc7
I1026 16:03:10.113329  3288 net.cpp:129] Top shape: 10 4096 (40960)
I1026 16:03:10.113332  3288 net.cpp:137] Memory required for data: 82824760
I1026 16:03:10.113343  3288 layer_factory.hpp:77] Creating layer relu7
I1026 16:03:10.113351  3288 net.cpp:84] Creating Layer relu7
I1026 16:03:10.113356  3288 net.cpp:406] relu7 <- fc7
I1026 16:03:10.113363  3288 net.cpp:367] relu7 -> fc7 (in-place)
I1026 16:03:10.113373  3288 net.cpp:122] Setting up relu7
I1026 16:03:10.113376  3288 net.cpp:129] Top shape: 10 4096 (40960)
I1026 16:03:10.113379  3288 net.cpp:137] Memory required for data: 82988600
I1026 16:03:10.113382  3288 layer_factory.hpp:77] Creating layer drop7
I1026 16:03:10.113389  3288 net.cpp:84] Creating Layer drop7
I1026 16:03:10.113391  3288 net.cpp:406] drop7 <- fc7
I1026 16:03:10.113399  3288 net.cpp:367] drop7 -> fc7 (in-place)
I1026 16:03:10.113405  3288 net.cpp:122] Setting up drop7
I1026 16:03:10.113409  3288 net.cpp:129] Top shape: 10 4096 (40960)
I1026 16:03:10.113412  3288 net.cpp:137] Memory required for data: 83152440
I1026 16:03:10.113415  3288 layer_factory.hpp:77] Creating layer fc8
I1026 16:03:10.113422  3288 net.cpp:84] Creating Layer fc8
I1026 16:03:10.113425  3288 net.cpp:406] fc8 <- fc7
I1026 16:03:10.113432  3288 net.cpp:380] fc8 -> fc8
I1026 16:03:10.121309  3288 net.cpp:122] Setting up fc8
I1026 16:03:10.121331  3288 net.cpp:129] Top shape: 10 1000 (10000)
I1026 16:03:10.121335  3288 net.cpp:137] Memory required for data: 83192440
I1026 16:03:10.121342  3288 layer_factory.hpp:77] Creating layer prob
I1026 16:03:10.121352  3288 net.cpp:84] Creating Layer prob
I1026 16:03:10.121356  3288 net.cpp:406] prob <- fc8
I1026 16:03:10.121363  3288 net.cpp:380] prob -> prob
I1026 16:03:10.121377  3288 net.cpp:122] Setting up prob
I1026 16:03:10.121381  3288 net.cpp:129] Top shape: 10 1000 (10000)
I1026 16:03:10.121384  3288 net.cpp:137] Memory required for data: 83232440
I1026 16:03:10.121388  3288 net.cpp:200] prob does not need backward computation.
I1026 16:03:10.121392  3288 net.cpp:200] fc8 does not need backward computation.
I1026 16:03:10.121395  3288 net.cpp:200] drop7 does not need backward computation.
I1026 16:03:10.121399  3288 net.cpp:200] relu7 does not need backward computation.
I1026 16:03:10.121402  3288 net.cpp:200] fc7 does not need backward computation.
I1026 16:03:10.121407  3288 net.cpp:200] drop6 does not need backward computation.
I1026 16:03:10.121410  3288 net.cpp:200] relu6 does not need backward computation.
I1026 16:03:10.121414  3288 net.cpp:200] fc6 does not need backward computation.
I1026 16:03:10.121417  3288 net.cpp:200] pool5 does not need backward computation.
I1026 16:03:10.121423  3288 net.cpp:200] relu5 does not need backward computation.
I1026 16:03:10.121425  3288 net.cpp:200] conv5 does not need backward computation.
I1026 16:03:10.121429  3288 net.cpp:200] relu4 does not need backward computation.
I1026 16:03:10.121433  3288 net.cpp:200] conv4 does not need backward computation.
I1026 16:03:10.121438  3288 net.cpp:200] relu3 does not need backward computation.
I1026 16:03:10.121441  3288 net.cpp:200] conv3 does not need backward computation.
I1026 16:03:10.121446  3288 net.cpp:200] pool2 does not need backward computation.
I1026 16:03:10.121451  3288 net.cpp:200] norm2 does not need backward computation.
I1026 16:03:10.121456  3288 net.cpp:200] relu2 does not need backward computation.
I1026 16:03:10.121459  3288 net.cpp:200] conv2 does not need backward computation.
I1026 16:03:10.121464  3288 net.cpp:200] pool1 does not need backward computation.
I1026 16:03:10.121467  3288 net.cpp:200] norm1 does not need backward computation.
I1026 16:03:10.121471  3288 net.cpp:200] relu1 does not need backward computation.
I1026 16:03:10.121475  3288 net.cpp:200] conv1 does not need backward computation.
I1026 16:03:10.121479  3288 net.cpp:200] data does not need backward computation.
I1026 16:03:10.121482  3288 net.cpp:242] This network produces output prob
I1026 16:03:10.121497  3288 net.cpp:255] Network initialization done.
I1026 16:03:10.364275  3288 upgrade_proto.cpp:46] Attempting to upgrade input file specified using deprecated transformation parameters: alexnet.caffemodel
I1026 16:03:10.364300  3288 upgrade_proto.cpp:49] Successfully upgraded file specified using deprecated data transformation parameters.
W1026 16:03:10.364305  3288 upgrade_proto.cpp:51] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1026 16:03:10.364307  3288 upgrade_proto.cpp:55] Attempting to upgrade input file specified using deprecated V1LayerParameter: alexnet.caffemodel
I1026 16:03:10.612104  3288 upgrade_proto.cpp:63] Successfully upgraded file specified using deprecated V1LayerParameter
I1026 16:03:10.658936  3288 net.cpp:744] Ignoring source layer loss
Type                 Name                                          Param               Output
----------------------------------------------------------------------------------------------
Input                data                                             --    (10, 3, 227, 227)
Convolution          conv1                               (96, 3, 11, 11)     (10, 96, 55, 55)
LRN                  norm1                                            --     (10, 96, 55, 55)
Pooling              pool1                                            --     (10, 96, 27, 27)
Convolution          conv2                               (256, 48, 5, 5)    (10, 256, 27, 27)
LRN                  norm2                                            --    (10, 256, 27, 27)
Pooling              pool2                                            --    (10, 256, 13, 13)
Convolution          conv3                              (384, 256, 3, 3)    (10, 384, 13, 13)
Convolution          conv4                              (384, 192, 3, 3)    (10, 384, 13, 13)
Convolution          conv5                              (256, 192, 3, 3)    (10, 256, 13, 13)
Pooling              pool5                                            --      (10, 256, 6, 6)
InnerProduct         fc6                                    (4096, 9216)     (10, 4096, 1, 1)
InnerProduct         fc7                                    (4096, 4096)     (10, 4096, 1, 1)
InnerProduct         fc8                                    (1000, 4096)     (10, 1000, 1, 1)
Softmax              prob                                             --     (10, 1000, 1, 1)
Converting data...
Saving data...
Saving source...
Done.
